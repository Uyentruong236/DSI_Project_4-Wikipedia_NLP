{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "import pymongo\n",
    "import json\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.en import English\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.en import STOP_WORDS\n",
    "nlp = English()\n",
    "stop = set(stopwords.words('english'))\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient('54.201.199.246', 27016)\n",
    "\n",
    "wiki_db = client.wikipedia\n",
    "\n",
    "wiki_col = wiki_db.my_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['admin', 'local', 'my_database', 'test', 'wikipedia'], ['my_collection'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.database_names(), wiki_db.collection_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5654"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_col.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = wiki_col.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_df = pd.DataFrame(list(cursor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Business software    4117\n",
       "Machine learning     1537\n",
       "Name: main_cat, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_df['main_cat'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>article</th>\n",
       "      <th>content</th>\n",
       "      <th>main_cat</th>\n",
       "      <th>page_id</th>\n",
       "      <th>sub_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5a15de5730b30c01325f0260</td>\n",
       "      <td>Business software</td>\n",
       "      <td>merge enterprise software date october softw...</td>\n",
       "      <td>Business software</td>\n",
       "      <td>1037763</td>\n",
       "      <td>Business software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5a15de5830b30c01325f0261</td>\n",
       "      <td>AccuSystems</td>\n",
       "      <td>multiple issue orphan date february notabili...</td>\n",
       "      <td>Business software</td>\n",
       "      <td>41270069</td>\n",
       "      <td>Business software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5a15de5830b30c01325f0262</td>\n",
       "      <td>Active policy management</td>\n",
       "      <td>active policy management business orient ent...</td>\n",
       "      <td>Business software</td>\n",
       "      <td>5211212</td>\n",
       "      <td>Business software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5a15de5830b30c01325f0263</td>\n",
       "      <td>Alexandria (library software)</td>\n",
       "      <td>use alexandria alexandria browser base softw...</td>\n",
       "      <td>Business software</td>\n",
       "      <td>28502793</td>\n",
       "      <td>Business software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5a15de5930b30c01325f0264</td>\n",
       "      <td>Alteryx</td>\n",
       "      <td>infobox company name alteryx logo file alter...</td>\n",
       "      <td>Business software</td>\n",
       "      <td>44133735</td>\n",
       "      <td>Business software</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id                        article  \\\n",
       "0  5a15de5730b30c01325f0260              Business software   \n",
       "1  5a15de5830b30c01325f0261                    AccuSystems   \n",
       "2  5a15de5830b30c01325f0262       Active policy management   \n",
       "3  5a15de5830b30c01325f0263  Alexandria (library software)   \n",
       "4  5a15de5930b30c01325f0264                        Alteryx   \n",
       "\n",
       "                                             content           main_cat  \\\n",
       "0    merge enterprise software date october softw...  Business software   \n",
       "1    multiple issue orphan date february notabili...  Business software   \n",
       "2    active policy management business orient ent...  Business software   \n",
       "3    use alexandria alexandria browser base softw...  Business software   \n",
       "4    infobox company name alteryx logo file alter...  Business software   \n",
       "\n",
       "    page_id            sub_cat  \n",
       "0   1037763  Business software  \n",
       "1  41270069  Business software  \n",
       "2   5211212  Business software  \n",
       "3  28502793  Business software  \n",
       "4  44133735  Business software  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_df.drop_duplicates(subset=['page_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use TIFIDF to vectorize words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(min_df = 20, stop_words = 'english')\n",
    "\n",
    "article_term_matrix_sps = tfidf_vectorizer.fit_transform(wiki_df.content)\n",
    "\n",
    "article_term_matrix_df = pd.DataFrame(article_term_matrix_sps.toarray(),\n",
    "                                       index=wiki_df.index,\n",
    "                                       columns=tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_term_matrix_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.concat([wiki_df.article, wiki_df.content, article_term_matrix_df], axis=1).sample(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use SVD to reduce number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 500\n",
    "SVD = TruncatedSVD(n_components)\n",
    "component_names = [\"component_\"+str(i+1) for i in range(n_components)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_matrix = SVD.fit_transform(article_term_matrix_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(SVD.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_df = pd.DataFrame(svd_matrix,\n",
    "                      index=article_term_matrix_df.index,\n",
    "                      columns=component_names)\n",
    "svd_df['article'] = wiki_df.article\n",
    "\n",
    "vocabulary_expression = pd.DataFrame(SVD.components_,\n",
    "                                     index=component_names,\n",
    "                                     columns=tfidf_vectorizer.get_feature_names()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>component_1</th>\n",
       "      <th>component_2</th>\n",
       "      <th>component_3</th>\n",
       "      <th>component_4</th>\n",
       "      <th>component_5</th>\n",
       "      <th>component_6</th>\n",
       "      <th>component_7</th>\n",
       "      <th>component_8</th>\n",
       "      <th>component_9</th>\n",
       "      <th>component_10</th>\n",
       "      <th>...</th>\n",
       "      <th>component_492</th>\n",
       "      <th>component_493</th>\n",
       "      <th>component_494</th>\n",
       "      <th>component_495</th>\n",
       "      <th>component_496</th>\n",
       "      <th>component_497</th>\n",
       "      <th>component_498</th>\n",
       "      <th>component_499</th>\n",
       "      <th>component_500</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.404951</td>\n",
       "      <td>-0.051309</td>\n",
       "      <td>-0.040997</td>\n",
       "      <td>0.105726</td>\n",
       "      <td>0.005325</td>\n",
       "      <td>0.123327</td>\n",
       "      <td>0.257478</td>\n",
       "      <td>-0.055066</td>\n",
       "      <td>-0.030250</td>\n",
       "      <td>0.069374</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010739</td>\n",
       "      <td>-0.010833</td>\n",
       "      <td>-0.023893</td>\n",
       "      <td>-0.023662</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>-0.016697</td>\n",
       "      <td>0.034757</td>\n",
       "      <td>0.009308</td>\n",
       "      <td>0.019910</td>\n",
       "      <td>Business software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.392458</td>\n",
       "      <td>-0.058191</td>\n",
       "      <td>-0.175200</td>\n",
       "      <td>0.280272</td>\n",
       "      <td>-0.069831</td>\n",
       "      <td>-0.002597</td>\n",
       "      <td>-0.034980</td>\n",
       "      <td>-0.052762</td>\n",
       "      <td>-0.030091</td>\n",
       "      <td>-0.091638</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014407</td>\n",
       "      <td>-0.008930</td>\n",
       "      <td>0.022893</td>\n",
       "      <td>-0.022598</td>\n",
       "      <td>0.018009</td>\n",
       "      <td>0.003248</td>\n",
       "      <td>0.013680</td>\n",
       "      <td>0.011187</td>\n",
       "      <td>-0.004865</td>\n",
       "      <td>AccuSystems</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.182103</td>\n",
       "      <td>-0.024958</td>\n",
       "      <td>-0.010863</td>\n",
       "      <td>0.079786</td>\n",
       "      <td>0.076378</td>\n",
       "      <td>0.052743</td>\n",
       "      <td>0.067091</td>\n",
       "      <td>0.026575</td>\n",
       "      <td>-0.068131</td>\n",
       "      <td>0.055673</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016108</td>\n",
       "      <td>-0.009719</td>\n",
       "      <td>0.012636</td>\n",
       "      <td>-0.013827</td>\n",
       "      <td>0.012805</td>\n",
       "      <td>0.001512</td>\n",
       "      <td>0.005479</td>\n",
       "      <td>-0.000555</td>\n",
       "      <td>0.008461</td>\n",
       "      <td>Active policy management</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.222011</td>\n",
       "      <td>-0.035081</td>\n",
       "      <td>-0.032633</td>\n",
       "      <td>-0.016740</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>0.002462</td>\n",
       "      <td>-0.014987</td>\n",
       "      <td>-0.016434</td>\n",
       "      <td>-0.001359</td>\n",
       "      <td>0.003528</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006281</td>\n",
       "      <td>0.015953</td>\n",
       "      <td>-0.012347</td>\n",
       "      <td>0.000610</td>\n",
       "      <td>0.000705</td>\n",
       "      <td>-0.024892</td>\n",
       "      <td>0.001886</td>\n",
       "      <td>-0.020234</td>\n",
       "      <td>0.024531</td>\n",
       "      <td>Alexandria (library software)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.328022</td>\n",
       "      <td>-0.045848</td>\n",
       "      <td>-0.136050</td>\n",
       "      <td>0.251924</td>\n",
       "      <td>-0.101908</td>\n",
       "      <td>-0.008932</td>\n",
       "      <td>-0.081420</td>\n",
       "      <td>-0.045797</td>\n",
       "      <td>-0.059878</td>\n",
       "      <td>-0.074248</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011901</td>\n",
       "      <td>0.003449</td>\n",
       "      <td>-0.022862</td>\n",
       "      <td>-0.007964</td>\n",
       "      <td>0.026287</td>\n",
       "      <td>0.029398</td>\n",
       "      <td>-0.009497</td>\n",
       "      <td>-0.012136</td>\n",
       "      <td>-0.020458</td>\n",
       "      <td>Alteryx</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   component_1  component_2  component_3  component_4  component_5  \\\n",
       "0     0.404951    -0.051309    -0.040997     0.105726     0.005325   \n",
       "1     0.392458    -0.058191    -0.175200     0.280272    -0.069831   \n",
       "2     0.182103    -0.024958    -0.010863     0.079786     0.076378   \n",
       "3     0.222011    -0.035081    -0.032633    -0.016740     0.000543   \n",
       "4     0.328022    -0.045848    -0.136050     0.251924    -0.101908   \n",
       "\n",
       "   component_6  component_7  component_8  component_9  component_10  \\\n",
       "0     0.123327     0.257478    -0.055066    -0.030250      0.069374   \n",
       "1    -0.002597    -0.034980    -0.052762    -0.030091     -0.091638   \n",
       "2     0.052743     0.067091     0.026575    -0.068131      0.055673   \n",
       "3     0.002462    -0.014987    -0.016434    -0.001359      0.003528   \n",
       "4    -0.008932    -0.081420    -0.045797    -0.059878     -0.074248   \n",
       "\n",
       "               ...                component_492  component_493  component_494  \\\n",
       "0              ...                    -0.010739      -0.010833      -0.023893   \n",
       "1              ...                    -0.014407      -0.008930       0.022893   \n",
       "2              ...                    -0.016108      -0.009719       0.012636   \n",
       "3              ...                    -0.006281       0.015953      -0.012347   \n",
       "4              ...                    -0.011901       0.003449      -0.022862   \n",
       "\n",
       "   component_495  component_496  component_497  component_498  component_499  \\\n",
       "0      -0.023662       0.003200      -0.016697       0.034757       0.009308   \n",
       "1      -0.022598       0.018009       0.003248       0.013680       0.011187   \n",
       "2      -0.013827       0.012805       0.001512       0.005479      -0.000555   \n",
       "3       0.000610       0.000705      -0.024892       0.001886      -0.020234   \n",
       "4      -0.007964       0.026287       0.029398      -0.009497      -0.012136   \n",
       "\n",
       "   component_500                        article  \n",
       "0       0.019910              Business software  \n",
       "1      -0.004865                    AccuSystems  \n",
       "2       0.008461       Active policy management  \n",
       "3       0.024531  Alexandria (library software)  \n",
       "4      -0.020458                        Alteryx  \n",
       "\n",
       "[5 rows x 501 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,11):\n",
    "    vocabulary_expression['abs_component_{}'.format(i)] = \\\n",
    "    np.abs(vocabulary_expression['component_{}'.format(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocabulary_expression['abs_component_1'].sort_values(ascending=False).head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create function to search for top 5 related articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_for_pages(search_terms):\n",
    "    '''\n",
    "    Takes search terms and returns the top 5 articles within the wikipedia corpus \n",
    "    that relate to that search terms based on cosine similarity.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    search_terms: str\n",
    "    A string of words  \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A Dataframe of the top 5 articles with the highest cosine similarities.\n",
    "     \n",
    "    '''\n",
    "    \n",
    "    temp_svd_df = svd_df.copy()\n",
    "    \n",
    "    search_terms = [search_terms]\n",
    "\n",
    "    search_terms_encoded = tfidf_vectorizer.transform(search_terms)\n",
    "    \n",
    "    search_term_svd_vector = SVD.transform(search_terms_encoded)\n",
    "    \n",
    "    temp_svd_df['cosine_sim'] = cosine_similarity(temp_svd_df.drop('article', axis=1), search_term_svd_vector)\n",
    "    \n",
    "    return temp_svd_df[['article', 'cosine_sim']].sort_values('cosine_sim', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>cosine_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2879</th>\n",
       "      <td>Investor application</td>\n",
       "      <td>0.833884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1625</th>\n",
       "      <td>Alpha capture system</td>\n",
       "      <td>0.393927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>Fundamental Analysis Software</td>\n",
       "      <td>0.382128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>PandaDoc</td>\n",
       "      <td>0.371676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1643</th>\n",
       "      <td>FatKat (investment software)</td>\n",
       "      <td>0.345342</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            article  cosine_sim\n",
       "2879           Investor application    0.833884\n",
       "1625           Alpha capture system    0.393927\n",
       "109   Fundamental Analysis Software    0.382128\n",
       "181                        PandaDoc    0.371676\n",
       "1643   FatKat (investment software)    0.345342"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_for_pages('There are two types of investor apps: Native investor apps and HTML5 investor apps. Most investor apps offer access to public company content such as stock quotes, corporate materials')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>cosine_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4882</th>\n",
       "      <td>AAAI Conference on Artificial Intelligence</td>\n",
       "      <td>0.821924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4884</th>\n",
       "      <td>Conference on Artificial General Intelligence</td>\n",
       "      <td>0.697031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4887</th>\n",
       "      <td>Dartmouth workshop</td>\n",
       "      <td>0.665428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4891</th>\n",
       "      <td>International Joint Conference on Artificial I...</td>\n",
       "      <td>0.624944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4334</th>\n",
       "      <td>Language Acquisition Device (computer)</td>\n",
       "      <td>0.613826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                article  cosine_sim\n",
       "4882         AAAI Conference on Artificial Intelligence    0.821924\n",
       "4884      Conference on Artificial General Intelligence    0.697031\n",
       "4887                                 Dartmouth workshop    0.665428\n",
       "4891  International Joint Conference on Artificial I...    0.624944\n",
       "4334             Language Acquisition Device (computer)    0.613826"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_for_pages('Artificial intelligence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create models to predict categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "wiki_df['cat_numerical'] = le.fit_transform(wiki_df['main_cat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3052\n",
       "1    1087\n",
       "Name: cat_numerical, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_df['cat_numerical'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedShuffleSplit(n_splits=5, random_state=42, test_size=0.1,\n",
       "            train_size=None),\n",
       "       error_score='raise',\n",
       "       estimator=Pipeline(steps=[('vec', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "   ...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'vec__ngram_range': [(1, 2)], 'vec__min_df': [10, 30], 'svd__n_components': [100, 300, 500], 'clf__C': array([  1.00000e-02,   1.00000e-01,   1.00000e+00,   1.00000e+01,\n",
       "         1.00000e+02,   1.00000e+03,   1.00000e+04])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = wiki_df['content']\n",
    "y = wiki_df['cat_numerical']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state = 42)\n",
    "\n",
    "nlp_pipe = Pipeline([\n",
    "    ('vec', TfidfVectorizer()),\n",
    "#     ('scaler', StandardScaler(with_mean=False)), try with and without scaler and tell Sylvia\n",
    "    #add SVD to decrease dimensionality \n",
    "    #experiment with Bayes \n",
    "    ('svd', TruncatedSVD()),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'vec__ngram_range':[(1,2)],#don't need up to 4, too much \n",
    "    'vec__min_df':[10,30],\n",
    "    'svd__n_components':[100,300,500],\n",
    "    'clf__C': np.logspace(-2,4,7)\n",
    "}\n",
    "\n",
    "nlp_gs = GridSearchCV(nlp_pipe, \n",
    "                      params, \n",
    "                      cv=StratifiedShuffleSplit(5, random_state=42))\n",
    "\n",
    "nlp_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__C': 10000.0,\n",
       " 'svd__n_components': 300,\n",
       " 'vec__min_df': 10,\n",
       " 'vec__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99163987138263665"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97971014492753628"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_eval(model, X_test, y_test):\n",
    "    preds = model.predict(X_test)\n",
    "    return {\n",
    "        'roc_auc': roc_auc_score(y_test, preds),\n",
    "        'accuracy': accuracy_score(y_test, preds),\n",
    "        'confmat': confusion_matrix(y_test, preds),\n",
    "        'clf_rep': classification_report(y_test, preds)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "roc_auc\n",
      "0.971668714888\n",
      "------------------------------------------------------------\n",
      "accuracy\n",
      "0.979710144928\n",
      "------------------------------------------------------------\n",
      "confmat\n",
      "[[759   9]\n",
      " [ 12 255]]\n",
      "------------------------------------------------------------\n",
      "clf_rep\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.99      0.99       768\n",
      "          1       0.97      0.96      0.96       267\n",
      "\n",
      "avg / total       0.98      0.98      0.98      1035\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "results = score_eval(nlp_gs, X_test, y_test)\n",
    "\n",
    "print('-'*60)\n",
    "for key, value in results.items():\n",
    "    print(key)\n",
    "    print(value)\n",
    "    print('-'*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "\n",
    "nbayes = BernoulliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedShuffleSplit(n_splits=5, random_state=42, test_size=0.1,\n",
       "            train_size=None),\n",
       "       error_score='raise',\n",
       "       estimator=Pipeline(steps=[('vec', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "   ...te=None, tol=0.0)), ('clf', BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'vec__min_df': [10, 30], 'svd__n_components': [100, 300, 500]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = wiki_df['content']\n",
    "y = wiki_df['cat_numerical']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state = 42)\n",
    "\n",
    "nlp_pipe = Pipeline([\n",
    "    ('vec', TfidfVectorizer()),\n",
    "#     ('scaler', StandardScaler(with_mean=False)), try with and without scaler and tell Sylvia\n",
    "    #add SVD to decrease dimensionality \n",
    "    #experiment with Bayes \n",
    "    ('svd', TruncatedSVD()),\n",
    "    ('clf', BernoulliNB())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'vec__min_df':[10,30],\n",
    "    'svd__n_components':[100,300,500],\n",
    "}\n",
    "\n",
    "nlp_bayes_gs = GridSearchCV(nlp_pipe, \n",
    "                      params, \n",
    "                      cv=StratifiedShuffleSplit(5, random_state=42))\n",
    "\n",
    "nlp_bayes_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9198067632850242"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_bayes_gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "roc_auc\n",
      "0.88243943118\n",
      "------------------------------------------------------------\n",
      "accuracy\n",
      "0.919806763285\n",
      "------------------------------------------------------------\n",
      "confmat\n",
      "[[737  31]\n",
      " [ 52 215]]\n",
      "------------------------------------------------------------\n",
      "clf_rep\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.96      0.95       768\n",
      "          1       0.87      0.81      0.84       267\n",
      "\n",
      "avg / total       0.92      0.92      0.92      1035\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "bayes_results = score_eval(nlp_bayes_gs, X_test, y_test)\n",
    "\n",
    "print('-'*60)\n",
    "for key, value in bayes_results.items():\n",
    "    print(key)\n",
    "    print(value)\n",
    "    print('-'*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
