{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "import pymongo\n",
    "import json\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient('54.201.199.246', 27016)\n",
    "\n",
    "wiki_db_2 = client.wikipedia_2\n",
    "\n",
    "wiki_colllection = wiki_db_2.my_collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create functions to get data from Wiki API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def category_request(category):\n",
    "    \"\"\"\n",
    "    Scrape a category page from Wikipedia API.\n",
    "    \n",
    "    Params:\n",
    "    --------\n",
    "    category: str\n",
    "        The name of the category to be scraped.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame\n",
    "        Pandas DataFrame containing categories \n",
    "        \n",
    "    \"\"\"\n",
    "    my_params = {\n",
    "        'action':'query',\n",
    "        'format':'json',\n",
    "        'list':'categorymembers',\n",
    "        'cmtitle': 'Category:{}'.format(category),\n",
    "        'cmlimit': 'max'\n",
    "        }\n",
    "    page = requests.get('http://en.wikipedia.org/w/api.php', params=my_params)\n",
    "    return pd.DataFrame(page.json()['query']['categorymembers'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(title):\n",
    "    \"\"\"\n",
    "    Scrape a page from Wikipedia API to get the conecnt.\n",
    "    \n",
    "    Params:\n",
    "    --------\n",
    "    title: str\n",
    "        The name of the page to be scraped.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    List of dictionaries\n",
    "        list of the content of the page\n",
    "        \n",
    "    \"\"\"\n",
    "    my_params = {\n",
    "        'action':'query',\n",
    "        'format':'json',\n",
    "        'titles': title,\n",
    "        'prop': 'revisions',\n",
    "        'rvprop': 'content'\n",
    "    }\n",
    "    content = requests.get('http://en.wikipedia.org/w/api.php', params=my_params)\n",
    "    return list(content.json()['query']['pages'].values())[0]['revisions'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'revisions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-08c65118d902>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Machine learning portal'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-2fab91a3a632>\u001b[0m in \u001b[0;36mget_content\u001b[0;34m(title)\u001b[0m\n\u001b[1;32m     22\u001b[0m     }\n\u001b[1;32m     23\u001b[0m     \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'http://en.wikipedia.org/w/api.php'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmy_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'query'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pages'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'revisions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'revisions'"
     ]
    }
   ],
   "source": [
    "get_content('Machine learning portal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cats_and_pages(category):\n",
    "    \"\"\"\n",
    "    Returns the pages and subcategories of a category\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    category : str\n",
    "        Name of a category\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    sub_categories: list \n",
    "        list of sub categories \n",
    "    pages: list\n",
    "        list of pages on the category\n",
    "        \n",
    "    \"\"\"\n",
    "    cats = pd.DataFrame(category_request(category))\n",
    "    cats['title'] = cats.title.astype(str) \n",
    "    #returns a boolean mask of all titles with 'category' in the str\n",
    "    subs_mask = cats['title'].str.contains('Category:')\n",
    "    \n",
    "    #creates list of new sub catagories\n",
    "    children = list(cats['title'][subs_mask].str.replace('Category:', \"\"))\n",
    "    pages = list(cats['title'][~cats.title.str.contains('Category:')])\n",
    "    page_id = list(cats['pageid'][~cats.title.str.contains('Category:')])\n",
    "    return page_id, pages, children\n",
    "\n",
    "#sub_categories, pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wiki_traverse(category):\n",
    "    \"\"\" \n",
    "    Returns a list of dictionary of categories, page titles and page contents\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    category : str\n",
    "        Name of a category\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    page_content: list \n",
    "        list of dictionaries with categories, page titles and page contents \n",
    "        \n",
    "    \"\"\"\n",
    "        \n",
    "    q = []\n",
    "    q.append(category) \n",
    "\n",
    "    page_content = []\n",
    " \n",
    "    #while the q is not empty\n",
    "    while q: \n",
    "        current_node = q.pop(0) #pop the first element off the list you've created \n",
    "#         print(current_node)\n",
    "        \n",
    "        page_id, pages, children = get_cats_and_pages(current_node)\n",
    "        \n",
    "        for child in children:\n",
    "            q.append(child)   \n",
    "     \n",
    "        for index, article in enumerate(pages):    \n",
    "            article_dict = {}\n",
    "            article_dict['category'] = current_node\n",
    "            article_dict['article'] = article\n",
    "            article_dict['content'] = get_content(article)\n",
    "            article_dict['page_id'] = str(page_id[index])\n",
    "            \n",
    "            #this line adds each article onto mongo database as each article is being called \n",
    "#             wiki_colllection.insert_one(article_dict) \n",
    "            \n",
    "            page_content.append(article_dict)      \n",
    "            \n",
    "    return page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = wiki_traverse('Machine learning portal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'article': 'Portal:Machine learning/Related portals',\n",
       "  'category': 'Machine learning portal',\n",
       "  'content': {'*': '{{Related portals2|Artificial intelligence|Computer science|Information technology|Robotics|Statistics|Technology}}<noinclude>\\n[[Category:Machine learning portal|{{SUBPAGENAME}}]]\\n</noinclude>',\n",
       "   'contentformat': 'text/x-wiki',\n",
       "   'contentmodel': 'wikitext'},\n",
       "  'page_id': '44943378'},\n",
       " {'article': 'Portal:Machine learning/Selected biography',\n",
       "  'category': 'Machine learning portal',\n",
       "  'content': {'*': 'Selected persons/biography archive for the Machine learning portal. [[Portal:Machine learning]] is where you go to update the selected biography. Include a link below to the Wikipedia article that has become the new selected article (in reverse chronological order):\\n* [[Geoffrey Hinton]]\\n<noinclude>[[Category:Machine learning portal|{{SUBPAGENAME}}]]</noinclude>',\n",
       "   'contentformat': 'text/x-wiki',\n",
       "   'contentmodel': 'wikitext'},\n",
       "  'page_id': '44942962'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Look up article in Mongo Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['admin', 'local', 'my_database', 'test', 'wikipedia'], ['my_collection'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.database_names(), wiki_db.collection_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1636"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_col.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cursor = wiki_col.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'*': '{{Multiple issues|{{refimprove|date=July 2017}}{{more footnotes|date=July 2017}}}}\\n\\n\\'\\'\\'Data exploration\\'\\'\\' is an approach similar to initial data analysis, whereby a data analyst uses visual exploration to understand what is in a dataset and the characteristics of the data, rather than through traditional data management systems<ref name=\"Foster\">[https://www.fosteropenscience.eu/sites/default/files/pdf/2933.pdf FOSTER Open Science], Overview of Data Exploration Techniques: Stratos Idreos, Olga Papaemmonouil, Surajit Chaudhuri.</ref>. These characteristics can include size or amount of data, completeness of the data, correctness of the data, possible relationships amongst data elements or files/tables in the data.\\n\\nData exploration is typically conducted using a combination of automated and manual activities.<ref name=\"Foster\" /><ref name=\"Stanford2011\">[http://vis.stanford.edu/files/2011-Wrangler-CHI.pdf Stanford.edu], 2011 Wrangler: Interactive Visual Specification of Data Transformation Scripts, Kandel, Paepcke, Hellerstein Heer.</ref> Automated activities can include data profiling or data visualization or tabular reports to give the analyst an initial view into the data and an understanding of key characteristics.\\n\\nThis is often followed by manual drill-down or filtering of the data to identify anomalies or patterns identified through the automated actions.  Data exploration can also require manual scripting and queries into the data (e.g. using languages such as SQL or R) or using Excel or similar tools to view the raw data.<ref name=\"Stanford2012\">[http://vis.stanford.edu/files/2012-EnterpriseAnalysisInterviews-VAST.pdf Stanford.edu], IEEE Visual Analytics Science & Technology (VAST), Oct 2012 Enterprise Data Analysis and Visualization: An Interview Study., Sean Kandel, Andreas Paepcke, Joseph Hellerstein, Jeffrey Heer Proc.</ref>\\n\\nAll of these activities are aimed at creating a clear mental model and understanding of the data in the mind of the analyst, and defining basic metadata (statistics, structure, relationships) for the data set that can be used in further analysis.\\n\\nOnce this initial understanding of the data is had, the data can be pruned or refined by removing unusable parts of the data, correcting poorly formatted elements and defining relevant relationships across datasets<ref name=\"Stanford2011\" />. This process is also known as determining [[data quality]]<ref name=\"Stanford2012\" />.\\n\\nAt this stage, the data can be considered ready for deeper analysis or be handed off to other analysts or users who have specific needs for the data.\\n\\nData exploration can also refer to the adhoc querying and visualization of data to identify potential relationships or insights that may be hidden in the data<ref name=\"Foster\" />.  In this scenario, hypotheses may be created and then the data is explored to identify whether those hypotheses are correct. \\n\\nTraditionally, this had been a key area of focus for statisticians, with [[John Tukey]] being a key evangelist in the field. Today, data exploration is more widespread and is the focus of data analysts and [[data scientists]]; the latter being a relatively new role within enterprises and larger organizations.\\n\\n== Interactive Data Exploration ==\\nThis area of data exploration has become an area of interest in the field of machine learning. This is a relatively new field and is still evolving.<ref name=\"Stanford2012\" />  As it’s most basic level, a machine-learning algorithm can be fed a data set and can be used to identify whether a hypothesis is true based on the dataset. Common machine learning algorithms can focus on identifying specific patterns in the data.<ref name=\"Stanford2011\" /> Common patterns include regression, classification or clustering, but there are many possible patterns and algorithms that can be applied to data via machine learning.\\n\\nBy employing machine learning, it is possible to find patterns or relationships in the data that would be difficult or impossible to find via manual inspection, trial and error or traditional exploration techniques.\\n\\n==Software==\\n* [[Trifacta]] – a data preparation and analysis platform\\n* [[Paxata]] – self-service data preparation software\\n* [[Alteryx]] – data blending and advanced data analytics software\\n* IBM Infosphere Analyzer – a data profiling tool\\n* Microsoft [[Power BI]] -  interactive visualization and data analysis tool\\n* [[OpenRefine]] -  a standalone open source desktop application for data clean-up and data transformation \\n* [[Tableau software]] – interactive data visualization software\\n\\n==See also==\\n{{Portal|Information technology}}\\n* [[Exploratory Data Analysis]]\\n* [[Machine Learning]]\\n* [[Data profiling]]\\n* [[Data Visualization]]\\n{{-}}\\n\\n== References ==\\n{{reflist}}\\n\\n[[Category:Machine learning| ]]\\n[[Category:Data analysis]]\\n[[Category:Data management]]\\n[[Category:Data quality]]',\n",
       " 'contentformat': 'text/x-wiki',\n",
       " 'contentmodel': 'wikitext'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(cursor)['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
